{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "```{note}\n",
    "Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with simple(naive) independence assumptions between the features.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative and Generative Algorithms\n",
    "\n",
    "Discriminative：try to learn $p(y|x)$ directly, such as Logistic Regression.\n",
    "\n",
    "Generative：try to learn $p(x|y)$ and $p(y)$, such as Naive Bayes.\n",
    "\n",
    "When predicting, use bayes rule：\n",
    "\n",
    "$$p(y|x)=\\frac{p(x|y)p(y)}{p(x)}$$\n",
    "\n",
    "then：\n",
    "\n",
    "$$\\hat{y}=\\underset{y}{\\mbox{argmax}}\\ \\frac{p(x|y)p(y)}{p(x)}= \\underset{y}{\\mbox{argmax}}\\ {p(x|y)p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "Suppose $x=(x_{1}, x_{2},..., x_{d})$ and $x_{j}$  is binary, then：\n",
    "\n",
    "$$\n",
    "p(x|y) = p(x_{1},x_{2},...,x_{d}|y) = p(x_{1}|y)p(x_{2}|y,x_{1})...p(x_{d}|y,x_{1},...,x_{d-1})\n",
    "$$\n",
    "\n",
    "Naive bayes independent assumption：\n",
    "\n",
    "$$\n",
    "p(x_{1}|y)p(x_{2}|y,x_{1})...p(x_{d}|y,x_{1},...,x_{d-1})\n",
    "= p(x_{1}|y)p(x_{2}|y)...p(x_{d}|y)\n",
    "$$\n",
    "\n",
    "Parameterized by:\n",
    "\n",
    "$$y\\sim Bernoulli(\\phi)$$\n",
    "$$x_{j}|y=1 \\sim Bernoulli(\\phi_{j|y=1})$$\n",
    "$$x_{j}|y=0 \\sim Bernoulli(\\phi_{j|y=0})$$\n",
    "\n",
    "note:\n",
    "\n",
    "$$k = \\sum_{i=1}^{n}1\\left\\{y^{(i)}=1\\right\\}$$\n",
    "$$s_{j} = \\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=1\\right\\}$$\n",
    "$$t_{j} = \\sum_{i=1}^{n}1\\left\\{x_{j}^{(i)}=1 \\wedge y^{(i)}=0\\right\\}$$\n",
    "\n",
    "We can write down the joint log likelihood of the data:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\phi, \\phi_{j|y=1}, \\phi_{j|y=0}) &= log\\prod_{i=1}^{n}p(x^{(i)},y^{(i)})\\\\\n",
    "&=\\sum_{i=1}^{n}log(p(x^{(i)}, y^{(i)})) \\\\\n",
    "&=\\sum_{i=1}^{n}log(p(y^{(i)})\\prod_{j=1}^{d}p(x_{j}^{(i)}|y^{(i)})) \\\\\n",
    "&=\\sum_{y^{(i)}=1}(log(\\phi) + \\sum_{j=1}^{d}log(p(x_{j}^{(i)}|y=1))) + \\sum_{y^{(i)}=0}(log(1 - \\phi) + \\sum_{j=1}^{d}log(p(x_{j}^{(i)}|y=0))) \\\\\n",
    "&=k\\ log(\\phi) + (n-k)log(1 - \\phi) + \\sum_{j=1}^{d}(s_{j}\\ log(\\phi_{j|y=1}) + (k-s_{j})log(1 - \\phi_{j|y=1}) + t_{j}\\ log(\\phi_{j|y=0}) + (n -k - t_{j})log(1 - \\phi_{j|y=0})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Maximum likelihood estimate result:\n",
    "\n",
    "$$\\phi=\\frac{k}{n}$$\n",
    "\n",
    "$$\\phi_{j|y=1} = \\frac{s_{j}}{k}$$\n",
    "\n",
    "$$\\phi_{j|y=0} = \\frac{t_{j}}{n-k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "\n",
    "To predict unseen data, we add 1 to the numerator, add 2 to the denominator:\n",
    "\n",
    "$$\\phi_{j|y=1} = \\frac{s_{j} + 1}{k + 2}$$\n",
    "\n",
    "$$\\phi_{j|y=0} = \\frac{t_{j} + 1}{n-k + 2}$$\n",
    "\n",
    "It's actually Binary Bernoulli Naive Bayes and implemented in *BernoulliNB*.\n",
    "\n",
    "It assumes each feature is binary-valued, if handed any other kind of data, a *BernoulliNB* instance may binarize its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf.predict(X[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "*MultinomialNB* implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification.\n",
    "\n",
    "This distribution is parametrized by vectors $\\theta_{y} = (\\theta_{y1},...,\\theta_{yn})$ for each class $y$, where $n$ is the number of features(in text classification, the size of the vocabulary) and $\\theta_{yi} = p(x_{i}|y)$.\n",
    "\n",
    "The parameters $\\theta_{y}$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "\n",
    "$$\\hat{\\theta}_{yi} = \\frac{N_{yi} + \\alpha}{N_{y} + \\alpha{n}}$$\n",
    "\n",
    "where $N_{yi}$ is the number of times feature $i$ appears in a sample of class $y$, and $N_{y} = \\sum_{i=1}^{n}N_{yi}$, $\\alpha \\ge 0$ is smooth hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf.predict(X[2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "When $x_{j}$ is real-valued, we can assume the likelihood of the features is Gaussian:\n",
    "\n",
    "$$p(x_{j}|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{y}^2}}\\exp\\left(-\\frac{(x_{j} - \\mu_{y})^2}{2\\sigma_{y}^2}\\right)$$\n",
    "\n",
    "Just like the binary case, parameters $\\mu_{y}$ and $\\sigma_{y}$ are estimated using maximum likelihood.\n",
    "\n",
    "This is Gaussian Naive Bayes and implemented in *GaussianNB*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" \n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
