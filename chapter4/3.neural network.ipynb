{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "```{note}\n",
    "A neural network is a network or circuit of neurons, composed of artificial neurons or nodes.<br/>\n",
    "We will mainly discuss backpropagation here.<br/>\n",
    "We will discuss it in more detail in the course \"deep learning\".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron\n",
    "\n",
    "a neuron takes input $x \\in \\mathbb{R}^{d}$, multiply $x$ by weights $w$ and add bias term $b$, finally use a activation function $g$.\n",
    "\n",
    "that is:\n",
    "\n",
    "$$f(x) = g(w^{T}x + b)$$\n",
    "\n",
    "it is analogous to the functionality of biological neuron.\n",
    "\n",
    "![jupyter](../images/neuron.svg)\n",
    "\n",
    "some useful activation function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\text{sigmoid:}\\quad &g(z) = \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\text{tanh:}\\quad &g(z) = \\frac{e^{z}-e^{-z}}{e^{z} + e^{-z}} \\\\\n",
    "\\text{relu:}\\quad &g(z) = max(z,0) \\\\\n",
    "\\text{leaky relu:}\\quad &g(z) = max(z, \\epsilon{z})\\ ,\\ \\epsilon\\text{ is a small positive number}\\\\\n",
    "\\text{identity:}\\quad &g(z) = z\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "linear regression's forward process is a neuron with identity activation function.\n",
    "\n",
    "logistic regression's forward process is a neuron with sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "building neural network is analogous to lego bricks: you take individual bricks and stack them together to build complex structures.\n",
    "\n",
    "![jupyter](../images/mlp.svg)\n",
    "\n",
    "we use bracket to denote layer, we take the above as example\n",
    "\n",
    "$[0]$ denote input layer, $[1]$ denote hidden layer, $[2]$ denote output layer\n",
    "\n",
    "$a^{[l]}$ denote the output of layer $l$, set $a^{[0]} := x$\n",
    "\n",
    "$z^{[l]}$ denote the affine result of layer $l$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "where $W^{[l]} \\in \\mathbb{R}^{d[l] \\times d[l-1]}$, $b^{[l]} \\in \\mathbb{R}^{d[l]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequesities for Back-Propagation\n",
    "\n",
    "suppose in forward-propagation $x \\to y \\to l$, where $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R} ^{m}$, loss $l \\in \\mathbb{R}$.\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial y} = \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\frac{\\partial l}{\\partial x} = \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial x_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "by total differential equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial x_{k}} = \\sum_{j=1}^{m}\\frac{\\partial l}{\\partial y_{j}}\\frac{\\partial y_{j}}{\\partial x_{k}}\n",
    "$$\n",
    "\n",
    "then we can connect $\\frac{\\partial l}{\\partial x}$ and $\\frac{\\partial l}{\\partial y}$ by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial x} = \\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial x_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & ... & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots  & \\ddots  & \\vdots \\\\\n",
    "  \\frac{\\partial y_{1}}{\\partial x_{n}}& .... & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    " ...\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "(\\frac{\\partial y}{\\partial x})^{T}\\frac{\\partial l}{\\partial y}\n",
    "$$\n",
    "\n",
    "here $\\frac{\\partial y}{\\partial x}$ is the jacobian matrix.\n",
    "\n",
    "unlike other activation functions, calculate softmax depend on other neurons, so jacobian of softmax.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_{i}}{\\partial z_{j}} = \\frac{\\partial}{\\partial z_{j}}\\left(\\frac{exp(z_{i})}{\\sum_{s=1}^{k}exp(z_{s})}\\right)\n",
    "$$\n",
    "\n",
    "it is easy to check the jacobian of matrix-multiplication:\n",
    "\n",
    "$$\\frac{\\partial Mx}{\\partial x} = M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation\n",
    "\n",
    "gradient descent update rule:\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{W^{[l]}}}$$\n",
    "\n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\frac{\\partial{L}}{\\partial{b^{[l]}}}$$\n",
    "\n",
    "to proceed, we must compute the gradient with respect to the parameters.\n",
    "\n",
    "we can define a three-step recipe for computing the gradients as follows:\n",
    "\n",
    "1.for output layer, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\hat{y}, y)}{\\partial z^{[N]}} = (\\frac{\\partial \\hat{y}}{\\partial z^{[N]}})^{T}\\frac{\\partial L(\\hat{y}, y)}{\\partial \\hat{y}}\n",
    "$$\n",
    "\n",
    "if $g^{[N]}$ is softmax.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\hat{y}, y)}{\\partial z^{[N]}} = \\frac{\\partial L(\\hat{y}, y)}{\\partial \\hat{y}} \\odot {g^{[N]}}'(z^{[N]})\n",
    "$$\n",
    "\n",
    "if not softmax.\n",
    "\n",
    "the above computations are all straight forward.\n",
    "\n",
    "2.for $l=N-1,...,1$, we have:\n",
    "\n",
    "$$z^{[l + 1]} = W^{[l + 1]}a^{[l]} + b^{[l + 1]}$$\n",
    "\n",
    "so by our prerequesities:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a^{[l]}} = (\\frac{\\partial z^{[l+1]}}{\\partial a^{[l]}})^{T}\\frac{\\partial L}{\\partial z^{[l+1]}} = (W^{[l+1]})^{T}\\frac{\\partial L}{\\partial z^{[l+1]}}\n",
    "$$\n",
    "\n",
    "we also have:\n",
    "\n",
    "$$a^{[l]} = g^{[l]}z^{[l]}$$\n",
    "\n",
    "we do not use softmax activation in hidden layers, so the dependent is direct:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = \\frac{\\partial L}{\\partial a^{[l]}} \\odot {g^{[l]}}'(z^{[l]})$$\n",
    "\n",
    "combine two equations:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z^{[l]}} = (W^{[l+1]})^{T}\\frac{\\partial L}{\\partial z^{[l+1]}} \\odot {g^{[l]}}'(z^{[l]})$$\n",
    "\n",
    "3.final step, because:\n",
    "\n",
    "$$z^{[l]} = W^{[l]}a^{[l - 1]} + b^{[l]}$$\n",
    "\n",
    "so:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial z^{[l]}}(a^{[l - 1]})^{T}$$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}}=\\frac{\\partial L}{\\partial z^{[l]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.02858299, 0.97141701]]), 0.96)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"mlp classification\"\"\"\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "# stratify=y makes sure train & test set have the same positive proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50),\n",
    "                    activation=\"relu\",\n",
    "                    max_iter=300)\n",
    "clf.fit(X_train, y_train)\n",
    "# score return the mean accuracy on the given test data and labels.\n",
    "clf.predict_proba(X_test[:1]), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15.80479452, 30.59838355]), 0.5456425414797952)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"mlp regression\"\"\"\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=200, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "regr = MLPRegressor(hidden_layer_sizes=(128, 64),\n",
    "                    solver='adam', \n",
    "                    max_iter=1000)\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# return 1 - u=(y_true - y_pred)**2 / v=(y_true - y_true.mean())**2\n",
    "regr.predict(X_test[:2]), regr.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
