{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN\n",
    "\n",
    "KNN(K-Nearest Neighbors) is a non-parametric classification method used for classification and regression.\n",
    "\n",
    "The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these.\n",
    "\n",
    "* In KNN classification, the output is a class membership. An object is classified by a vote of its neighbors.\n",
    "\n",
    "* In KNN regression, the output is the average of the values of $k$ nearest neighbors.\n",
    "\n",
    "The optimal choice of the value $k$ is highly data-dependent: in general, a larger $k$ suppresses the effects of noise, but makes the classification boundaries less distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([[0.66666667, 0.33333333]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"KNN classification\"\"\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3, \n",
    "                           algorithm=\"kd_tree\")\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf.predict([[1.1]]), clf.predict_proba([[0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"KNN regression\"\"\"\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "reg = KNeighborsRegressor(n_neighbors=2,\n",
    "                          weights=\"uniform\")\n",
    "reg.fit(X, y)\n",
    "\n",
    "reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDTree\n",
    "\n",
    "A k-d tree (short for k-dimensional tree) is a space-partitioning data structure for organizing points in a k-dimensional space.\n",
    "\n",
    "It's useful in searching nearest neighors in KNN.\n",
    "\n",
    "```{image} ../images/kdtree.png\n",
    ":alt: kdtree\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "As the above fiture, k-d tree first split dataset through y-axis into two equivalent subsets, then split each subset through x-axis, and so on.\n",
    "\n",
    "When searching for nearest neighors, k-d tree first locate the new instance's leaf, then only search in and near that leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
