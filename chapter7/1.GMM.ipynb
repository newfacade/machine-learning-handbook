{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model\n",
    "\n",
    "```{note}\n",
    "GMM assumes instances were generated from a mixture of several Gaussian distributions.<br/>\n",
    "GMM uses the EM algorithm, E-step estimates which Gaussian, M-step maximizes parameters.<br/>\n",
    "We can use BIC and AIC to select the number of distributions.\n",
    "```\n",
    "\n",
    "The simplest GMM variant which implemented in *GaussianMixture* must know in advance the number of Gaussian distributions $k$.\n",
    "\n",
    "GMM assumes instances are generated by two steps:\n",
    "\n",
    "$$z^{(i)} \\sim \\mbox{Multinomial}(\\phi)$$\n",
    "\n",
    "$$x^{(i)}|z^{(i)}=j \\sim \\mathcal{N}(\\mu_{j}, \\Sigma_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate\n",
    "\n",
    "The parameters of our model are $\\phi$, $\\mu$ and $\\Sigma$. To estimate them, we can write down the likelihood of our data:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{l}(\\phi, \\mu, \\Sigma) &= \\sum_{i=1}^{n}\\log{p(x^{(i)};\\phi,\\mu,\\Sigma)}\\\\\n",
    "&= \\sum_{i=1}^{n}\\log\\sum_{z^{(i)}=1}^{k}p(x^{(i)}|z^{(i)}; \\mu,\\Sigma)p(z^{(i)}; \\phi)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "No closed form solution to this MLE problem, while if we knew what  $z^{(i)}$'s were, the problem would have been easy.\n",
    "\n",
    "The EM algorithm iteratively estimates $z^{(i)}$ and maximizes parameters:\n",
    "\n",
    "1. the E-step tries to estimate $z^{(i)}$, more precisely:\n",
    "\n",
    "$$\n",
    "w_{j}^{(i)} := p(z^{(i)}=j|x^{(i)}; \\phi,\\mu,\\Sigma)\n",
    "$$\n",
    "\n",
    "2. the M-step maximizes the parameters of based on our guesses:\n",
    "\n",
    "$$\n",
    "\\phi_{j} = \\frac{1}{n}\\sum_{i=1}^{n}w_{j}^{(i)}\n",
    "$$\n",
    "$$\n",
    "\\mu_{j} = \\frac{\\sum_{i=1}^{n}w_{j}^{(i)}x^{(i)}}{\\sum_{i=1}^{n}w_{j}^{(i)}}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{j} = \\frac{\\sum_{i=1}^{n}w_{j}^{(i)}(x^{(i)} - \\mu_{j})(x^{(i)} - \\mu_{j})^{T}}{\\sum_{i=1}^{n}w_{j}^{(i)}}\n",
    "$$\n",
    "\n",
    "```{image} ../images/gmm.png\n",
    ":alt: gmm\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "# need to assign n_components\n",
    "gm = GaussianMixture(n_components=2, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# phi\n",
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.,  2.],\n",
       "       [ 1.,  2.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mu\n",
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00000000e-06, 1.15699599e-29],\n",
       "        [1.15699599e-29, 2.66666767e+00]],\n",
       "\n",
       "       [[1.00000000e-06, 1.24902977e-30],\n",
       "        [1.24902977e-30, 2.66666767e+00]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigma\n",
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict = np.array([[0, 1], [11, 5]])\n",
    "gm.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict_proba(X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9.99844709,  3.21101447],\n",
       "        [ 9.99866645, -1.70416514],\n",
       "        [ 9.99803008,  0.62383712],\n",
       "        [ 1.00050588,  4.06756329],\n",
       "        [ 0.99891919,  6.15629625],\n",
       "        [ 0.99942086,  2.79087858],\n",
       "        [ 0.9985898 ,  1.70347684]]),\n",
       " array([0, 0, 0, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as a generative model, we can sample new instances from it\n",
    "X_new, y_new = gm.sample(7)\n",
    "X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.40556655, 0.42448816, 1.59092314, 2.95683262, 0.06321552,\n",
       "       3.60133524, 2.87549155])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log of probability density function(PDF)\n",
    "gm.score_samples(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Number of Distributions\n",
    "\n",
    "GMM use metrics such as *Bayesian information criterion* (BIC) or the *Akaike information criterion* (AIC):\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "BIC &= \\log(n)p - 2\\log(\\hat{L}) \\\\\n",
    "AIC &= 2p - 2\\log(\\hat{L})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of instances, $p$ is the number of parameters, $\\hat{L}$ is the likelihood.\n",
    "\n",
    "Both BIC and AIC penalize models that have more parameters to learn and reward models that fit the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.92644270457478"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "select k that minimize bic or the elbow of bic.\n",
    "\"\"\"\n",
    "gm.bic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.635796866083382"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.aic(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
