{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "```{note}\n",
    "Principal Component Analysis(PCA) is by far the most popular dimentionality reduction algorithm.\n",
    "\n",
    "First it identifies the hyperplane that lies closest to the data, then projects the data onto that hyperplane.\n",
    "```\n",
    "\n",
    "To choose the right hyperplane, PCA select the axis that preserve the maximum amount of variance, called the 1-th principal component.\n",
    "\n",
    "It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance, called the 2-th principal component, and so on.\n",
    "\n",
    "Maximize variance $\\Leftrightarrow$ Minimize the mean squared distance, because when we count translation:\n",
    "\n",
    "$$\\mbox{axis variance} + \\mbox{mean square distance to axis} = \\mbox{variance(which is constant)}$$\n",
    "\n",
    "```{image} ../images/pca.png\n",
    ":alt: pca\n",
    ":width: 500px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Given a  $m\\times{n}$ matrix $\\mathbf{X}$, we can decompose it as:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^{T}$$\n",
    "\n",
    "where $\\mathbf{U}$, $\\mathbf{V}$ is orthogonal, $\\mathbf{\\Sigma}$ is diagonal.\n",
    "\n",
    "principal components of $\\mathbf{X}$ $\\Leftrightarrow$ columns of $\\mathbf{V}$ $\\Leftrightarrow$ eigenvectors of covariance matrix of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "manual dataset\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "m = 100\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.2\n",
    "\n",
    "X = np.empty((m, 3))\n",
    "X[:, :2] = np.random.multivariate_normal([0, 0], [[2, 1], [1, 5]], m)\n",
    "# almost linear dependent\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# fit + transform\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76240779, 0.23294253])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-th PCA variance ratio, 2-th PCA variance ratio, almost add up to 1\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dimension that just preserve over 95% of variance\n",
    "after training\n",
    "\"\"\"\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set n_components to indicate the ratio of variance you wish to preserve\n",
    "no need to run again\n",
    "\"\"\"\n",
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA\n",
    "\n",
    "Incremental PCA split the training set into mini-batches and feed them one mini-batch at a time.\n",
    "\n",
    "This is useful for large training sets and for applying PCA online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 5\n",
    "inc_pca = IncrementalPCA(n_components=2)\n",
    "# split X_train into 5 batches\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    # partial_fit instead of fit\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "\n",
    "Using the kernel trick, KernelPCA is good at preserving clusters of instances after projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# rbf kernel is applied\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Dimensionality Reduction\n",
    "\n",
    "As Dimensionality Reduction is unsupervised, there is no abvious performance measure.\n",
    "\n",
    "While dimentionality reduction is often a preparation step for a supervised learning task.\n",
    "\n",
    "So we can use grid search to select the hyperparameters that lead to the best performance on that task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
