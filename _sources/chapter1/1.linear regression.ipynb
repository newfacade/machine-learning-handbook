{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "```{note}\n",
    "Linear Regression = Linear Model + Mean Square Loss<br/>\n",
    "Linear Regression has nice geometric and probabilistic Interpretations.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Suppose $x \\in \\mathbb{R}^{d}$, $y \\in \\mathbb{R}$. Linear model is:\n",
    "\n",
    "$$h(x) = w^{T}x + b$$\n",
    "\n",
    "For simplicity, let:\n",
    "\n",
    "$$x := [x,1] \\in \\mathbb{R}^{d + 1}$$\n",
    "\n",
    "$$\\theta := [w, b] \\in \\mathbb{R}^{d + 1}$$\n",
    "\n",
    "Then linear model could be write as:\n",
    "\n",
    "$$h(x) = \\theta^{T}x$$\n",
    "\n",
    "![jupyter](../images/linear_regression1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Loss Function is mean square loss:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{n}(h(x^{(i)}) - y^{(i)})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Rule\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "$$\\theta \\to \\theta - \\alpha\\nabla{J(\\theta)}$$\n",
    "\n",
    "Gradient of Linear Regression:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split} \n",
    "\\frac{\\partial }{\\partial \\theta_{j}}J(\\theta) &=  \\frac{\\partial }{\\partial \\theta}_{j}\\frac{1}{2}\\sum_{i=1}^{n}(h(x^{(i)}) - y^{(i)})^2  \\\\ \n",
    "&=\\sum_{i=1}^{n}(h(x^{(i)}) - y^{(i)})\\cdot\\frac{\\partial }{\\partial \\theta_{j}}(h(x^{(i)}) - y^{(i)})\\\\ \n",
    "& =\\sum_{i=1}^{n}(h(x^{(i)}) - y^{(i)})\\cdot{x_{j}}^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Combine all dimensions:\n",
    "\n",
    "$$\\theta \\to \\theta - \\alpha\\sum_{i=1}^{n}(h(x^{(i)}) - y^{(i)})\\cdot{x^{(i)}} $$\n",
    "\n",
    "Write in matrix form:\n",
    "\n",
    "$$\\theta \\to \\theta - \\alpha{X^{T}}(X\\theta-y) $$\n",
    "\n",
    "where $X \\in \\mathbb{R}^{n\\times{d}}, y \\in \\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Solution\n",
    "\n",
    "From above, we have:\n",
    "\n",
    "$$\\nabla{J(\\theta)} = X^{T}X\\theta - X^{T}y$$\n",
    "\n",
    "If $X^{T}X$ is invertible:\n",
    "\n",
    "$$\\theta = (X^{T}X)^{-1}X^{T}y$$\n",
    "\n",
    "Else the equation also has a solution:\n",
    "\n",
    "$$X^{T}X\\theta=0 \\Rightarrow \\theta^{T}X^{T}X\\theta = (X\\theta)^{T}X\\theta=0 \\Rightarrow X\\theta = 0$$\n",
    "\n",
    "$$\\mbox{null}(X^{T}X) = \\mbox{null}(X) \\Rightarrow \\mbox{range}(X^{T}X) = \\mbox{range}(X^{T})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.894831181729202"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y, reg.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Interpretation\n",
    "\n",
    "Denote linear space $S = \\mbox{span}\\left \\{\\mbox{columns of } X \\right \\}$, linear combination of $S$ should be written as $X\\theta$.\n",
    "\n",
    "$X\\theta$ is the projection of $y$ on $S \\Leftrightarrow$ $X\\theta - y$ orthogonal with $S \\Leftrightarrow$ orthogonal with columns of $X \\Leftrightarrow X^{T}(X\\theta - y)=0$\n",
    "\n",
    "Linear regression $\\Leftrightarrow$ Finding the projection of $y$ on $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Interpretation\n",
    "\n",
    "Assume targets and inputs are related via:\n",
    "\n",
    "$$y^{(i)} = \\theta^{T}x^{(i)} + \\epsilon^{(i)}$$\n",
    "\n",
    "where $\\epsilon^{(i)}$ is the error term and distributed IID according to Gaussian with mean 0 and variance $\\sigma^{2}$:\n",
    "\n",
    "$$p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left (-\\frac{(\\epsilon^{(i)})^{2}}{2\\sigma^{2}}\\right )$$\n",
    "\n",
    "This is equivalent to say(we should denote that $\\theta$ is not a random variable here):\n",
    "\n",
    "$$p(y^{(i)}|x^{(i)}; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right)$$\n",
    "\n",
    "The likelihood function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\theta) &= \\prod_{i=1}^{n}p(y^{(i)}|x^{(i)}; \\theta) \\\\\n",
    "&= \\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Maximize the log likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\log(L(\\theta)) &= \\log\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right) \\\\\n",
    "&= \\sum_{i=1}^{n}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left ( -\\frac{(y^{(i)} - \\theta^{T}x^{(i)})^{2}}{2\\sigma^{2}}\\right) \\\\\n",
    "&= n\\log\\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}x^{(i)})^{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "hence, maximizing the log likelihood gives the same answer as minimizing:\n",
    "\n",
    "$$\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)} - \\theta^{T}x^{(i)})^{2} = J(\\theta)$$\n",
    "\n",
    "Linear regression $\\Leftrightarrow $ Maximum Likelihood Estimate given Gaussian error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
