{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "```{note}\n",
    "Logistic Regression = Logistic model + binary cross entropy loss.<br/>\n",
    "For multi-class classification problem, we can use softmax regression.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For binary classification problem where $x \\in \\mathbb{R}^{d}$, $y \\in \\left\\{0, 1\\right\\}$, we could approach the classification problem using linear regression ignoring the fact that $y$ is discrete. Howerver, it is easy to construct examples that performs poorly, it doesn't make sense for $h(x)$ outside $\\left[0, 1\\right]$.\n",
    "\n",
    "To fix this, we used the logistic function(sigmoid) to force the result in $\\left[0, 1\\right]$ after the affine transformation:\n",
    "\n",
    "$$h(x) = \\frac{1}{1 + \\mbox{exp}(-\\theta^{T}x)} = \\sigma(\\theta^{T}x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Self information $I(x)$ indicates the amount of information an event $x$ to happen that satisfies:\n",
    "\n",
    "1. $I(x) \\ge 0$\n",
    "2. $\\text{if }p(x_{1}) > p(x_{2}) \\text{, then } I(x_{1}) < I(x_{2})$\n",
    "3. $I(x_{1}, x_{2}) = I(x_{1}) + I(x_{2}) \\text{ for independent }x_{1},x_{2}$\n",
    "\n",
    "This leads to $I(x) = -\\log_{r}p(x)$, for convenient $I(x) := -\\log{p(x)}$.\n",
    "\n",
    "While self-information measures the information of a event, entropy measures the information of a random variable:\n",
    "\n",
    "$$\n",
    "H(X) = E(I(x)) = E(-\\log{p(x)}) = -\\sum_{x \\in \\mathcal{X}}\\log{p(x)}\n",
    "$$\n",
    "\n",
    "It is exactly the optimal encoding length of $X$.\n",
    "\n",
    "Cross entropy $H(p, q)$ is the encoding length of $p$ by optimal encoding of $q$:\n",
    "\n",
    "$$H(p,q)=E_{p}\\left[-\\log{q(x)}\\right] = -\\sum_{x}p(x)\\log{q(x)}$$\n",
    "\n",
    "Fix $p$, the closer $q$ is to $p$, the less is $H(p,q)$. We can use $H(p,q)$ to define the distance from $q$ to $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Using the definition of cross entropy above, we interpret label $y^{(i)}$ as the distribution $p(y^{(i)}|x^{(i)}) = 1, p(1 - y^{(i)}|x^{(i)})=0$.\n",
    "\n",
    "In the same manner, interpret the hypothesis as $q(y=1|x^{(i)}) = h(x^{(i)}), q(y=0|x^{(i)}) = 1 - h(x^{(i)})$.\n",
    "\n",
    "Cross Entropy Loss from $q$ to $p$ measures the distance from hypothesis to label:\n",
    "\n",
    "$$l_{\\theta}(x^{(i)}) = -y^{(i)}\\log(h(x^{(i)})) - (1 - y^{(i)})\\log(1 - h(x^{(i)}))$$\n",
    "\n",
    "Sum them up derive the cross entropy loss logistic regression uses:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^{n}\\left[-y^{(i)}\\log(h(x^{(i)})) - (1 - y^{(i)})\\log(1 - h(x^{(i)}))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Interpretation\n",
    "\n",
    "As we supposes:\n",
    "\n",
    "$$p(y|x) = h(x)^{y}\\cdot(1 - h(x))^{1 - y} $$\n",
    "\n",
    "Log likelihood of the dataset:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\theta) &= \\log\\prod_{i=1}^{n} h(x^{(i)})^{y^{(i)}}\\cdot(1 - h(x^{(i)}))^{1 - y^{(i)}} \\\\\n",
    "&= \\sum_{i=1}^{n}y^{(i)}\\log(h(x^{(i)})) + (1 - y^{(i)})\\log(1 - h(x^{(i)}))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "So logistic regression $\\Leftrightarrow$ MLE if we see $h(x)$ as $p(y=1|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Rule\n",
    "\n",
    "Gradient of logistic regression:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial }{\\partial \\theta_{j}}J(\\theta ) \n",
    "&= \\sum_{i=1}^{n} \\left (-y^{(i)}\\frac{1}{\\sigma(\\theta^{T}x^{(i)})} + (1 - y^{(i)})\\frac{1}{1 - \\sigma(\\theta^{T}x^{(i)})} \\right )\\frac{\\partial }{\\partial \\theta_{j}}\\sigma(\\theta^{T}x^{(i)})\\\\\n",
    "&=\\sum_{i=1}^{n} \\left (-y^{(i)}\\frac{1}{\\sigma(\\theta^{T}x^{(i)})} + (1 - y^{(i)})\\frac{1}{1 - \\sigma(\\theta^{T}x^{(i)})} \\right )\\sigma(\\theta^{T}x^{(i)})(1-\\sigma(\\theta^{T}x^{(i)}))\\frac{\\partial }{\\partial \\theta_{j}}\\theta^{T}x^{(i)} \\\\\n",
    "&=\\sum_{i=1}^{n}(h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Combine all dimensions:\n",
    "\n",
    "$$\\theta \\to \\theta - \\alpha\\sum_{i=1}^{n}(h(x^{(i)}) - y^{(i)})\\cdot{x}^{(i)} $$\n",
    "\n",
    "Write in matrix form:\n",
    "\n",
    "$$\\theta \\to \\theta - \\alpha{X}^{T}(\\sigma({X}{\\theta})-{y}) $$\n",
    "\n",
    "where ${X} \\in \\mathbb{R}^{n\\times{d}}, {y} \\in \\mathbb{R}^{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=5000)\n",
    "clf.fit(X, y)\n",
    "clf.predict(X[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 3.16211740e-14],\n",
       "        [9.99996140e-01, 3.86002382e-06]]),\n",
       " 0.9578207381370826)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score return the mean accuracy on the given test data and labels.\n",
    "clf.predict_proba(X[:2, :]), clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "For multi-class classification, we start off with a simple image classification problem, each input consists of a $2\\times{2}$ grayscale image, represent each pixel with a scalar, giving us features $\\left\\{x_{1},x_{2},x_{3}, x_{4}\\right\\}$. assume each image belong to one among the categories \"cat\", \"chiken\" and \"dog\".\n",
    "\n",
    "We have a nice way to represent categorical data: the one-hot encoding, for our problem, \"cat\" represents by $(1,0,0)$, \"chicken\" by $(0, 1, 0)$, \"dog\" by $(0, 0, 1)$.\n",
    "\n",
    "To estimate the conditional probabilities of all classes, we need a model with multiple outputs, one per class:\n",
    "\n",
    "$$o_{1} = x_{1}w_{11} + x_{2}w_{12} + x_{3}w_{13} + x_{4}w_{14}$$\n",
    "$$o_{2} = x_{1}w_{21} + x_{2}w_{22} + x_{3}w_{23} + x_{4}w_{24}$$\n",
    "$$o_{3} = x_{1}w_{31} + x_{2}w_{32} + x_{3}w_{33} + x_{4}w_{34}$$\n",
    "\n",
    "depict as:\n",
    "\n",
    "![jupyter](../images/softmaxreg.svg)\n",
    "\n",
    "We would like $\\hat{y}_{j}$ to be interpreted as probability that a given item belong to class $j$, to transform our current outputs $\\left\\{o_{1},o_{2},o_{3},o_{4}\\right\\}$ to probability distribution $\\left\\{\\hat{y}_{1},\\hat{y}_{2},\\hat{y}_{3},\\hat{y}_{4}\\right\\}$, just use the softmax operation:\n",
    "\n",
    "$$\\hat{y}_{j} = \\frac{\\exp(o_{j})}{\\sum_{k}\\exp(o_{k})}$$\n",
    "\n",
    "As logistic regression, we use the cross entropy loss:\n",
    "\n",
    "$$H(y,\\hat{y}) = -\\sum_{k}y_{j}\\log\\hat{y}_{j} = -\\log\\hat{y}_{\\text{category of y}}$$\n",
    "\n",
    "Now complete the construction of softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"multi-class classification problem\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set multi_class='multinomial' in LogisticRegression\n",
    "\"\"\"\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, max_iter=1000)\n",
    "softmax_reg.fit(X, y)\n",
    "softmax_reg.predict(X[:3, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
