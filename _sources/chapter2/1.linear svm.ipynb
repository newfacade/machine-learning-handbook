{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM\n",
    "\n",
    "```{note}\n",
    "The intuition behind Support Vector Machine(SVM) is to separate positive&negative samples by large margins.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin\n",
    "\n",
    "Suppose we have a binary classification problem with $x \\in \\mathbb{R}^{d}, y \\in \\{-1, 1\\}$. Try to solve this by linear classifier $h(x) = w^{T}x + b$, if $h(x) >= 0$ predict 1, else -1. Intuitively, in addition to correctly separate the data, we want to separate the data by large margins.\n",
    "\n",
    "```{image} ../images/margin.png\n",
    ":alt: margin\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Geometric margin of the classifier:\n",
    "\n",
    "$$\\gamma^{(i)}=\\frac{y^{(i)}(w^{T}x^{(i)} + b)}{\\left \\| w \\right \\| }$$\n",
    "\n",
    "We want to maximize geometic margin:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\gamma, w, b}{\\max}\\ &\\gamma \\\\\n",
    "\\mbox{s.t.}\\ &\\frac{y^{(i)}(w^{T}x^{(i)} + b)}{\\left \\| w \\right \\| } >= \\gamma\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Without loss of generality, set $\\gamma\\left \\| w \\right \\|=1$, then the above is equivalent to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w, b}{\\min}\\ &\\frac{1}{2}{\\left \\| w \\right \\|}^2 \\\\\n",
    "\\mbox{s.t.}\\ &{y^{(i)}(w^{T}x^{(i)} + b)} >= 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Problem\n",
    "\n",
    "We can write the constraints as:\n",
    "\n",
    "$$g_{i}(w) = 1 - y^{(i)}(w^{T}x^{(i)} + b) \\le 0$$\n",
    "\n",
    "The generalized Lagrangian of the above optimal problem:\n",
    "\n",
    "$$\n",
    "L(w,b,\\alpha )=\\frac{1}{2}\\left \\| w \\right \\|^{2} + \\sum_{i=1}^{n}\\alpha_{i}\\left [ 1 - y^{(i)}(w^{T}x^{(i)} + b) \\right ] \n",
    "$$\n",
    "\n",
    "Primal problem:\n",
    "\n",
    "$$\\underset{w, b}{\\min}\\ \\underset{\\alpha_{i}\\ge0}{\\max}\\ L(w, b, \\alpha)$$\n",
    "\n",
    "The KKT condition:\n",
    "\n",
    "1. $\\frac{1}{2}\\left \\| w \\right \\|^{2}$ is convex.\n",
    "2. $g_{i}$'s are convex.\n",
    "3. the constraints $g_{i}$'s are strictly feasible.\n",
    "\n",
    "is satisfied in the separable case, so the primal problem is equivalent to the dual problem:\n",
    "\n",
    "$$\\underset{\\alpha_{i}\\ge0}{\\max}\\ \\underset{w, b}{\\min}\\ L(w, b, \\alpha)$$\n",
    "\n",
    "Minimize $\\nabla_{w}L(w, b, \\alpha)$:\n",
    "\n",
    "$$\\nabla_{w}L(w, b, \\alpha) = w - \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)} = 0 \\Rightarrow w = \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}x^{(i)}$$\n",
    "\n",
    "$$\\nabla_{b}L(w, b, \\alpha) = \\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0$$\n",
    "\n",
    "Plug back to Lagrangian:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(w, b, \\alpha) =& \\frac{1}{2}\\left \\| w \\right \\|^{2} + \\sum_{i=1}^{n}\\alpha_{i}\\left [ 1 - y^{(i)}(w^{T}x^{(i)} + b)\\right ]\\\\\n",
    "=& \\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}(x^{(i)})^{T}x^{(i)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{\\max}\\ &\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "\\mbox{s.t.}\\ &\\alpha_{i}\\ge{0}, i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-separable Case\n",
    "\n",
    "For non-separable case, we add the soft term $\\xi$, thus transform the optimization:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{w, b}{\\min}\\ &\\frac{1}{2}{\\left \\| w \\right \\|}^2 + C\\sum_{i=1}^{n}\\xi_{i}\\\\\n",
    "\\mbox{s.t.}\\ &{y^{(i)}(w^{T}x^{(i)} + b)} >= 1 - \\xi_{i},\\ i=1,...,n\\\\\n",
    "&\\xi_{i} \\ge 0,\\ i=1,...,n.\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Lagrangian:\n",
    "\n",
    "$$\n",
    "L(w,b,\\xi,\\alpha,r )=\\frac{1}{2}\\left \\| w \\right \\|^{2} + C\\sum_{i=1}^{n}\\xi_{i} + \\sum_{i=1}^{n}\\alpha_{i}\\left [1 - \\xi -y^{(i)}(w^{T}x^{(i)} + b)\\right ] -\\sum_{i=1}^{n}r_{i}\\xi_{i}     \n",
    "$$\n",
    "\n",
    "Dual problem of the non-separable case:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{\\max}\\ &\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "\\mbox{s.t.}\\ &0 \\le \\alpha_{i}\\le{C},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"preprocessing + svm\"\"\"\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14144316, 0.52678399, 0.67978685, 0.49307524]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.named_steps['linearsvc'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMO algorithm\n",
    "\n",
    "We can not directly use coordinate ascent in SVM:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\underset{\\alpha}{\\max}\\ &\\sum_{i=1}^{n}\\alpha_{i} - \\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\\alpha_{i}\\alpha_{j}\\left \\langle x^{(i)},x^{(j)} \\right \\rangle \\\\\n",
    "\\mbox{s.t.}\\ &0 \\le \\alpha_{i}\\le{C},\\ i=1,...,n \\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y^{(i)}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If we want to update some $\\alpha_{i}$, we must update at least two of them simutaneously, coordinate ascent change to:\n",
    "\n",
    "1. select some pair $\\alpha_{i}, \\alpha_{j}$.\n",
    "2. optimize with respect to $\\alpha_{i}, \\alpha_{j}$ while holding other $\\alpha_{k}$ fixed.\n",
    "\n",
    "We can easily change step-2 into a quatratic optimization problem.\n",
    "\n",
    "Remaining the choice of $\\alpha_{i},\\alpha_{j}$ , in fact this step is heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
