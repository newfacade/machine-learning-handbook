{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM\n",
    "\n",
    "EM algorithm applies to a large faimily of estimation problems with latent variables, e.g GMM.\n",
    "\n",
    "Suppose we have a training set $\\{x^{(1)},...,x^{(n)}\\}$ with $z$ being the latent variable, by marginal probabilities:\n",
    "\n",
    "$$p(x;\\theta) = \\sum_{z}p(x, z; \\theta)$$\n",
    "\n",
    "We wish to fit the parameters $\\theta$ by maximizing the log-likelihood of the dataï¼š\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\theta) =& \\sum_{i=1}^{n}\\log{p(x^{(i)};\\theta)} \\\\\n",
    "=& \\sum_{i=1}^{n}\\log\\sum_{z^{(i)}}p(x^{(i)}, z^{(i)}; \\theta)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Maximizing $l(\\theta)$ directly might be difficult. \n",
    "\n",
    "Our strategy will be to instead repeatedly construct a lower-bound on $l$ (E-step), and then optimize that lower-bound (M-step)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Bound\n",
    "\n",
    "Let $Q$ be a distribution over $z$, then:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\log{p(x;\\theta)} =& \\log\\sum_{z}p(x,z;\\theta)\\\\\n",
    "=& \\log\\sum_{z}Q(z)\\frac{p(x,z;\\theta)}{Q(z)}\\\\\n",
    "\\ge& \\sum_{z}Q(z)\\log{\\frac{p(x,z;\\theta)}{Q(z)}}\\quad\\mbox{(log is concave)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We call this bound the *evidence lower bound(ELBO)* and denote it by:\n",
    "\n",
    "$$\\mbox{ELBO}(x;Q,\\theta) = \\sum_{z}Q(z)\\log\\frac{p(x,z;\\theta)}{Q(z)}$$\n",
    "\n",
    "To hold with equality, it is sufficient that:\n",
    "\n",
    "$$\\frac{p(x,z;\\theta)}{Q(z)} = c$$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$Q(z) = p(z|x;\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM Algorithm\n",
    "\n",
    "Taking all instances into account, for any distributions $Q_{1},...,Q_{n}$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "l(\\theta) \\ge& \\sum_{i}\\mbox{ELBO}(x^{(i)};Q_{i},\\theta)\\\\\n",
    "=& \\sum_{i}\\sum_{z^{(i)}}Q_{i}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_{i}(z^{(i)})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "equality holds when $Q_{i}$ equal to the posterior distribution in this setting of $\\theta$:\n",
    "\n",
    "$$\\mbox{E-step:}\\quad{Q_{i}{(z^{(i)})}} = p(z^{(i)}|x^{(i)};\\theta)$$\n",
    "\n",
    "M-step maximize the lower bound with respect to $\\theta$ while keeping $Q_{i}$ fixed.\n",
    "\n",
    "$$\n",
    "\\theta := \\underset{\\theta}{\\mbox{argmax}}\\sum_{i}\\mbox{ELBO}(x^{(i)};Q_{i},\\theta)\n",
    "$$\n",
    "\n",
    "EM algorithm ensures $l(\\theta^{(t)}) \\le l(\\theta^{(t+1)})$, thus ensures convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
